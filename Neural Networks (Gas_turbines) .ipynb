{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6716d0d",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f597b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2392/3287916388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbab04e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inmporting data\n",
    "data = pd.read_csv('gas_turbines.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b66812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      0\n",
       "AP      0\n",
       "AH      0\n",
       "AFDP    0\n",
       "GTEP    0\n",
       "TIT     0\n",
       "TAT     0\n",
       "TEY     0\n",
       "CDP     0\n",
       "CO      0\n",
       "NOX     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13707651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10689ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>17.764381</td>\n",
       "      <td>7.574323</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>11.408000</td>\n",
       "      <td>18.1860</td>\n",
       "      <td>23.8625</td>\n",
       "      <td>34.9290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>1013.199240</td>\n",
       "      <td>6.410760</td>\n",
       "      <td>985.850000</td>\n",
       "      <td>1008.900000</td>\n",
       "      <td>1012.8000</td>\n",
       "      <td>1016.9000</td>\n",
       "      <td>1034.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>82.2660</td>\n",
       "      <td>90.0435</td>\n",
       "      <td>100.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>4.1862</td>\n",
       "      <td>4.5509</td>\n",
       "      <td>7.6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>25.0820</td>\n",
       "      <td>27.1840</td>\n",
       "      <td>37.4020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIT</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>1083.798770</td>\n",
       "      <td>16.527806</td>\n",
       "      <td>1000.800000</td>\n",
       "      <td>1079.600000</td>\n",
       "      <td>1088.7000</td>\n",
       "      <td>1096.0000</td>\n",
       "      <td>1100.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>549.8900</td>\n",
       "      <td>550.0600</td>\n",
       "      <td>550.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>133.7800</td>\n",
       "      <td>140.8950</td>\n",
       "      <td>174.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDP</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>12.102353</td>\n",
       "      <td>1.103196</td>\n",
       "      <td>9.904400</td>\n",
       "      <td>11.622000</td>\n",
       "      <td>12.0250</td>\n",
       "      <td>12.5780</td>\n",
       "      <td>15.0810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>1.3902</td>\n",
       "      <td>2.1604</td>\n",
       "      <td>44.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>68.190934</td>\n",
       "      <td>10.470586</td>\n",
       "      <td>27.765000</td>\n",
       "      <td>61.303500</td>\n",
       "      <td>66.6010</td>\n",
       "      <td>73.9355</td>\n",
       "      <td>119.8900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count         mean        std          min          25%        50%  \\\n",
       "AT    15039.0    17.764381   7.574323     0.522300    11.408000    18.1860   \n",
       "AP    15039.0  1013.199240   6.410760   985.850000  1008.900000  1012.8000   \n",
       "AH    15039.0    79.124174  13.793439    30.344000    69.750000    82.2660   \n",
       "AFDP  15039.0     4.200294   0.760197     2.087400     3.723900     4.1862   \n",
       "GTEP  15039.0    25.419061   4.173916    17.878000    23.294000    25.0820   \n",
       "TIT   15039.0  1083.798770  16.527806  1000.800000  1079.600000  1088.7000   \n",
       "TAT   15039.0   545.396183   7.866803   512.450000   542.170000   549.8900   \n",
       "TEY   15039.0   134.188464  15.829717   100.170000   127.985000   133.7800   \n",
       "CDP   15039.0    12.102353   1.103196     9.904400    11.622000    12.0250   \n",
       "CO    15039.0     1.972499   2.222206     0.000388     0.858055     1.3902   \n",
       "NOX   15039.0    68.190934  10.470586    27.765000    61.303500    66.6010   \n",
       "\n",
       "            75%        max  \n",
       "AT      23.8625    34.9290  \n",
       "AP    1016.9000  1034.2000  \n",
       "AH      90.0435   100.2000  \n",
       "AFDP     4.5509     7.6106  \n",
       "GTEP    27.1840    37.4020  \n",
       "TIT   1096.0000  1100.8000  \n",
       "TAT    550.0600   550.6100  \n",
       "TEY    140.8950   174.6100  \n",
       "CDP     12.5780    15.0810  \n",
       "CO       2.1604    44.1030  \n",
       "NOX     73.9355   119.8900  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33706fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.207495</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.256744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.037299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIT</th>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>-0.231636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDP</th>\n",
       "      <td>-0.100705</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>-0.169103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.231636</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>-0.169103</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT        AP        AH      AFDP      GTEP       TIT       TAT  \\\n",
       "AT    1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.093067  0.338569   \n",
       "AP   -0.412953  1.000000  0.042573  0.040318  0.078575  0.029650 -0.223479   \n",
       "AH   -0.549432  0.042573  1.000000 -0.119249 -0.202784 -0.247781  0.010859   \n",
       "AFDP -0.099333  0.040318 -0.119249  1.000000  0.744251  0.627254 -0.571541   \n",
       "GTEP -0.049103  0.078575 -0.202784  0.744251  1.000000  0.874526 -0.756884   \n",
       "TIT   0.093067  0.029650 -0.247781  0.627254  0.874526  1.000000 -0.357320   \n",
       "TAT   0.338569 -0.223479  0.010859 -0.571541 -0.756884 -0.357320  1.000000   \n",
       "TEY  -0.207495  0.146939 -0.110272  0.717995  0.977042  0.891587 -0.720356   \n",
       "CDP  -0.100705  0.131198 -0.182010  0.727152  0.993784  0.887238 -0.744740   \n",
       "CO   -0.088588  0.041614  0.165505 -0.334207 -0.508259 -0.688272  0.063404   \n",
       "NOX  -0.600006  0.256744  0.143061 -0.037299 -0.208496 -0.231636  0.009888   \n",
       "\n",
       "           TEY       CDP        CO       NOX  \n",
       "AT   -0.207495 -0.100705 -0.088588 -0.600006  \n",
       "AP    0.146939  0.131198  0.041614  0.256744  \n",
       "AH   -0.110272 -0.182010  0.165505  0.143061  \n",
       "AFDP  0.717995  0.727152 -0.334207 -0.037299  \n",
       "GTEP  0.977042  0.993784 -0.508259 -0.208496  \n",
       "TIT   0.891587  0.887238 -0.688272 -0.231636  \n",
       "TAT  -0.720356 -0.744740  0.063404  0.009888  \n",
       "TEY   1.000000  0.988473 -0.541751 -0.102631  \n",
       "CDP   0.988473  1.000000 -0.520783 -0.169103  \n",
       "CO   -0.541751 -0.520783  1.000000  0.316743  \n",
       "NOX  -0.102631 -0.169103  0.316743  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bb253b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEY</th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114.70</td>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.72</td>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.71</td>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.72</td>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114.72</td>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>114.72</td>\n",
       "      <td>7.6998</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>92.708</td>\n",
       "      <td>3.5236</td>\n",
       "      <td>19.683</td>\n",
       "      <td>1059.8</td>\n",
       "      <td>549.97</td>\n",
       "      <td>10.626</td>\n",
       "      <td>3.4467</td>\n",
       "      <td>82.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>114.71</td>\n",
       "      <td>7.7901</td>\n",
       "      <td>1011.6</td>\n",
       "      <td>91.983</td>\n",
       "      <td>3.5298</td>\n",
       "      <td>19.659</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.644</td>\n",
       "      <td>3.4874</td>\n",
       "      <td>82.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114.71</td>\n",
       "      <td>7.7139</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>91.348</td>\n",
       "      <td>3.5088</td>\n",
       "      <td>19.673</td>\n",
       "      <td>1059.8</td>\n",
       "      <td>549.92</td>\n",
       "      <td>10.656</td>\n",
       "      <td>3.6043</td>\n",
       "      <td>83.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114.72</td>\n",
       "      <td>7.7975</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>90.196</td>\n",
       "      <td>3.5141</td>\n",
       "      <td>19.634</td>\n",
       "      <td>1060.1</td>\n",
       "      <td>550.09</td>\n",
       "      <td>10.644</td>\n",
       "      <td>3.3943</td>\n",
       "      <td>82.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>131.70</td>\n",
       "      <td>8.0820</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>88.597</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>23.406</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>550.21</td>\n",
       "      <td>11.679</td>\n",
       "      <td>1.9081</td>\n",
       "      <td>82.782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEY      AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP  \\\n",
       "0  114.70  6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605   \n",
       "1  114.72  6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598   \n",
       "2  114.71  6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601   \n",
       "3  114.72  7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606   \n",
       "4  114.72  7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612   \n",
       "5  114.72  7.6998  1010.7  92.708  3.5236  19.683  1059.8  549.97  10.626   \n",
       "6  114.71  7.7901  1011.6  91.983  3.5298  19.659  1060.0  549.87  10.644   \n",
       "7  114.71  7.7139  1012.7  91.348  3.5088  19.673  1059.8  549.92  10.656   \n",
       "8  114.72  7.7975  1013.8  90.196  3.5141  19.634  1060.1  550.09  10.644   \n",
       "9  131.70  8.0820  1015.0  88.597  4.0612  23.406  1083.0  550.21  11.679   \n",
       "\n",
       "       CO     NOX  \n",
       "0  3.1547  82.722  \n",
       "1  3.2363  82.776  \n",
       "2  3.2012  82.468  \n",
       "3  3.1923  82.670  \n",
       "4  3.2484  82.311  \n",
       "5  3.4467  82.409  \n",
       "6  3.4874  82.440  \n",
       "7  3.6043  83.010  \n",
       "8  3.3943  82.284  \n",
       "9  1.9081  82.782  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing position of target column 'TEY'\n",
    "Target = data['TEY']\n",
    "#Drop the existing column\n",
    "data.drop(labels=['TEY'], axis=1, inplace = True)\n",
    "data.insert(0, 'Y' , Target)\n",
    "\n",
    "data = data.rename({'Y':'TEY'}, axis = 1)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5869665e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEY</th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.256744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.110272</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>0.717995</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.037299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>0.977042</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIT</th>\n",
       "      <td>0.891587</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>-0.231636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>-0.720356</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.009888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDP</th>\n",
       "      <td>0.988473</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>-0.169103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.541751</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.102631</td>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.231636</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.169103</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEY        AT        AP        AH      AFDP      GTEP       TIT  \\\n",
       "TEY   1.000000 -0.207495  0.146939 -0.110272  0.717995  0.977042  0.891587   \n",
       "AT   -0.207495  1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.093067   \n",
       "AP    0.146939 -0.412953  1.000000  0.042573  0.040318  0.078575  0.029650   \n",
       "AH   -0.110272 -0.549432  0.042573  1.000000 -0.119249 -0.202784 -0.247781   \n",
       "AFDP  0.717995 -0.099333  0.040318 -0.119249  1.000000  0.744251  0.627254   \n",
       "GTEP  0.977042 -0.049103  0.078575 -0.202784  0.744251  1.000000  0.874526   \n",
       "TIT   0.891587  0.093067  0.029650 -0.247781  0.627254  0.874526  1.000000   \n",
       "TAT  -0.720356  0.338569 -0.223479  0.010859 -0.571541 -0.756884 -0.357320   \n",
       "CDP   0.988473 -0.100705  0.131198 -0.182010  0.727152  0.993784  0.887238   \n",
       "CO   -0.541751 -0.088588  0.041614  0.165505 -0.334207 -0.508259 -0.688272   \n",
       "NOX  -0.102631 -0.600006  0.256744  0.143061 -0.037299 -0.208496 -0.231636   \n",
       "\n",
       "           TAT       CDP        CO       NOX  \n",
       "TEY  -0.720356  0.988473 -0.541751 -0.102631  \n",
       "AT    0.338569 -0.100705 -0.088588 -0.600006  \n",
       "AP   -0.223479  0.131198  0.041614  0.256744  \n",
       "AH    0.010859 -0.182010  0.165505  0.143061  \n",
       "AFDP -0.571541  0.727152 -0.334207 -0.037299  \n",
       "GTEP -0.756884  0.993784 -0.508259 -0.208496  \n",
       "TIT  -0.357320  0.887238 -0.688272 -0.231636  \n",
       "TAT   1.000000 -0.744740  0.063404  0.009888  \n",
       "CDP  -0.744740  1.000000 -0.520783 -0.169103  \n",
       "CO    0.063404 -0.520783  1.000000  0.316743  \n",
       "NOX   0.009888 -0.169103  0.316743  1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09689ac4",
   "metadata": {},
   "source": [
    "# Feature selecting by using mutual information Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea4c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,1:]\n",
    "y = data.iloc[:, 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1270144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX  \n",
       "0      82.722  \n",
       "1      82.776  \n",
       "2      82.468  \n",
       "3      82.670  \n",
       "4      82.311  \n",
       "...       ...  \n",
       "15034  79.559  \n",
       "15035  79.917  \n",
       "15036  90.912  \n",
       "15037  93.227  \n",
       "15038  92.498  \n",
       "\n",
       "[15039 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3d0677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        114.70\n",
       "1        114.72\n",
       "2        114.71\n",
       "3        114.72\n",
       "4        114.72\n",
       "          ...  \n",
       "15034    111.61\n",
       "15035    111.78\n",
       "15036    110.19\n",
       "15037    110.74\n",
       "15038    111.58\n",
       "Name: TEY, Length: 15039, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed3f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "def select_feature(x_train, y_train,x_test):\n",
    "    #configure to selet all features\n",
    "    fs = SelectKBest(score_func = mutual_info_regression,k = 'all')\n",
    "    #Learn relationship from training data\n",
    "    fs.fit(x_train,y_train)\n",
    "    #transform train input data\n",
    "    x_train_fs = fs.transform(x_train)\n",
    "    #tranform test input data\n",
    "    x_test_fs = fs.transform(x_test)\n",
    "    return x_train_fs,x_test_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d6dc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1)\n",
    "#feature selection\n",
    "x_train_fs, x_test_fs, fs = select_feature(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d413a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.402915\n",
      "Feature 1: 0.144169\n",
      "Feature 2: 0.091262\n",
      "Feature 3: 0.657082\n",
      "Feature 4: 1.590716\n",
      "Feature 5: 1.305785\n",
      "Feature 6: 0.915088\n",
      "Feature 7: 1.710316\n",
      "Feature 8: 0.506246\n",
      "Feature 9: 0.302783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#What are the scores for the feature \n",
    "for i in range(len(fs.scores_)): \n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "#plot the scores\n",
    "fig, ax = plt.subplots(figsize= (12,8))\n",
    "plt.bar([i for i in range(len(fs.scores_))],fs.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2eb706",
   "metadata": {},
   "source": [
    "As per above feature selection method, we will select only features with good score to build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de042141",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['TEY', 'AT', 'AP','AH', 'CO', 'NOX'], axis=1)\n",
    "y = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dea6457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AFDP    GTEP     TIT     TAT     CDP\n",
       "0      3.5000  19.663  1059.2  550.00  10.605\n",
       "1      3.4998  19.728  1059.3  550.00  10.598\n",
       "2      3.4824  19.779  1059.4  549.87  10.601\n",
       "3      3.4805  19.792  1059.6  549.99  10.606\n",
       "4      3.4976  19.765  1059.7  549.98  10.612\n",
       "...       ...     ...     ...     ...     ...\n",
       "15034  3.5421  19.164  1049.7  546.21  10.400\n",
       "15035  3.5059  19.414  1046.3  543.22  10.433\n",
       "15036  3.4770  19.530  1037.7  537.32  10.483\n",
       "15037  3.4486  19.377  1043.2  541.24  10.533\n",
       "15038  3.4275  19.306  1049.9  545.85  10.583\n",
       "\n",
       "[15039 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9b6608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        114.70\n",
       "1        114.72\n",
       "2        114.71\n",
       "3        114.72\n",
       "4        114.72\n",
       "          ...  \n",
       "15034    111.61\n",
       "15035    111.78\n",
       "15036    110.19\n",
       "15037    110.74\n",
       "15038    111.58\n",
       "Name: TEY, Length: 15039, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78fc427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0094df3",
   "metadata": {},
   "source": [
    "Neural network model - backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5137839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e1ba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=5,activation='relu', input_shape=(5,)))\n",
    "model.add(Dense(6, activation= 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fed49384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb407b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "252/252 [==============================] - 8s 7ms/step - loss: 18290.0352 - mse: 18290.0352 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 2s 6ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 2s 7ms/step - loss: 18290.0195 - mse: 18290.0195 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 2s 6ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 1s 6ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 1s 5ms/step - loss: 18290.0234 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 1s 5ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0293 - mse: 18290.0293 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0137 - mse: 18290.0137 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0137 - mse: 18290.0137 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0156 - mse: 18290.0156 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0293 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0293 - mse: 18290.0293 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0352 - mse: 18290.0352 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0195 - mse: 18290.0195 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0293 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0215 - mse: 18290.0215 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0195 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0312 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0156 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0195 - mse: 18290.0195 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0156 - mse: 18290.0156 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0312 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0195 - mse: 18290.0195 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0293 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0312 - mse: 18290.0312 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0234 - mse: 18290.0234 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0254 - mse: 18290.0254 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0273 - mse: 18290.0273 - val_loss: 17380.0645 - val_mse: 17380.0645\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 18290.0176 - mse: 18290.0176 - val_loss: 17380.0645 - val_mse: 17380.0645\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "history = model.fit(x, y, validation_split=0.33,epochs=100, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d34c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4963, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_fs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8db1fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 2s 3ms/step - loss: 17989.7324 - mse: 17989.7324\n",
      "mse: 1798973.24%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(x, y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536b0da",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65db6af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AFDP    GTEP     TIT     TAT     CDP\n",
       "0      3.5000  19.663  1059.2  550.00  10.605\n",
       "1      3.4998  19.728  1059.3  550.00  10.598\n",
       "2      3.4824  19.779  1059.4  549.87  10.601\n",
       "3      3.4805  19.792  1059.6  549.99  10.606\n",
       "4      3.4976  19.765  1059.7  549.98  10.612\n",
       "...       ...     ...     ...     ...     ...\n",
       "15034  3.5421  19.164  1049.7  546.21  10.400\n",
       "15035  3.5059  19.414  1046.3  543.22  10.433\n",
       "15036  3.4770  19.530  1037.7  537.32  10.483\n",
       "15037  3.4486  19.377  1043.2  541.24  10.533\n",
       "15038  3.4275  19.306  1049.9  545.85  10.583\n",
       "\n",
       "[15039 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(['TEY','AT', 'AP','AH','CO','NOX'], axis= 1)\n",
    "Y =data.iloc[:,0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df55f7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        114.70\n",
       "1        114.72\n",
       "2        114.71\n",
       "3        114.72\n",
       "4        114.72\n",
       "          ...  \n",
       "15034    111.61\n",
       "15035    111.78\n",
       "15036    110.19\n",
       "15037    110.74\n",
       "15038    111.58\n",
       "Name: TEY, Length: 15039, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "390eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "X_standardized = a.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98237c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>3.810001e-16</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-2.779497</td>\n",
       "      <td>-0.626693</td>\n",
       "      <td>-0.018541</td>\n",
       "      <td>0.461220</td>\n",
       "      <td>4.486233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>1.107344e-16</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-1.806771</td>\n",
       "      <td>-0.509146</td>\n",
       "      <td>-0.080757</td>\n",
       "      <td>0.422864</td>\n",
       "      <td>2.871006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>-2.324212e-15</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-5.021933</td>\n",
       "      <td>-0.254051</td>\n",
       "      <td>0.296554</td>\n",
       "      <td>0.738249</td>\n",
       "      <td>1.028678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>1.744899e-15</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-4.188141</td>\n",
       "      <td>-0.410115</td>\n",
       "      <td>0.571257</td>\n",
       "      <td>0.592868</td>\n",
       "      <td>0.662784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15039.0</td>\n",
       "      <td>3.640356e-16</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-1.992416</td>\n",
       "      <td>-0.435434</td>\n",
       "      <td>-0.070119</td>\n",
       "      <td>0.431168</td>\n",
       "      <td>2.700105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count          mean       std       min       25%       50%       75%  \\\n",
       "0  15039.0  3.810001e-16  1.000033 -2.779497 -0.626693 -0.018541  0.461220   \n",
       "1  15039.0  1.107344e-16  1.000033 -1.806771 -0.509146 -0.080757  0.422864   \n",
       "2  15039.0 -2.324212e-15  1.000033 -5.021933 -0.254051  0.296554  0.738249   \n",
       "3  15039.0  1.744899e-15  1.000033 -4.188141 -0.410115  0.571257  0.592868   \n",
       "4  15039.0  3.640356e-16  1.000033 -1.992416 -0.435434 -0.070119  0.431168   \n",
       "\n",
       "        max  \n",
       "0  4.486233  \n",
       "1  2.871006  \n",
       "2  1.028678  \n",
       "3  0.662784  \n",
       "4  2.700105  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_standardized).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479d761",
   "metadata": {},
   "source": [
    "# Tuning of all hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c89dbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d56abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    " model = Sequential()\n",
    " model.add(Dense(neuron1,input_dim = 5,kernel_initializer = init,activation = activation_function))\n",
    " model.add(Dropout(dropout_rate))\n",
    " model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
    " model.add(Dropout(dropout_rate))\n",
    " model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    " adam = Adam(lr =learning_rate)\n",
    " model.compile(loss = 'mean_squared_error',optimizer = adam,metrics = ['mse'])\n",
    " return model\n",
    "\n",
    "#Create the model\n",
    "model= KerasClassifier(build_fn= create_model, verbose =0)\n",
    "\n",
    "#Define the grid search parameters\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "activation_function = ['softmax','relu', 'tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    " activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n",
      "[CV 1/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time= 1.0min\n",
      "[CV 2/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  35.3s\n",
      "[CV 3/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  35.3s\n",
      "[CV 4/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  33.5s\n",
      "[CV 5/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  33.3s\n",
      "[CV 1/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  33.5s\n",
      "[CV 2/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  32.8s\n",
      "[CV 3/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  33.3s\n",
      "[CV 4/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  33.2s\n",
      "[CV 5/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  34.8s\n",
      "[CV 1/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  35.4s\n",
      "[CV 2/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  33.9s\n",
      "[CV 3/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  33.8s\n",
      "[CV 4/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  35.5s\n",
      "[CV 5/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  35.2s\n",
      "[CV 1/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  34.2s\n",
      "[CV 2/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  43.6s\n",
      "[CV 3/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  33.2s\n",
      "[CV 4/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 4/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  23.7s\n",
      "[CV 5/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  22.6s\n",
      "[CV 1/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=nan total time=  22.2s\n",
      "[CV 2/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=nan total time=  22.5s\n",
      "[CV 3/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=nan total time=  23.6s\n",
      "[CV 4/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 4/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=nan total time=  22.2s\n",
      "[CV 5/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=nan total time=  22.6s\n",
      "[CV 1/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=nan total time=  35.7s\n",
      "[CV 2/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=nan total time=  22.4s\n",
      "[CV 3/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=nan total time=  29.5s\n",
      "[CV 4/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=nan total time=  26.8s\n",
      "[CV 5/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=nan total time=  23.7s\n",
      "[CV 1/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 1/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=nan total time=  22.0s\n",
      "[CV 2/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=nan total time=  21.3s\n",
      "[CV 3/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=nan total time=  24.2s\n",
      "[CV 4/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=nan total time=  23.5s\n",
      "[CV 5/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=nan total time=  23.5s\n",
      "[CV 1/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 1/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=nan total time=  23.0s\n",
      "[CV 2/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=nan total time=  23.3s\n",
      "[CV 3/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=nan total time=  26.4s\n",
      "[CV 4/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=nan total time=  35.6s\n",
      "[CV 5/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 5/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=nan total time=  32.8s\n",
      "[CV 1/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=nan total time=  32.3s\n",
      "[CV 2/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=nan total time=  34.4s\n",
      "[CV 3/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=nan total time=  33.6s\n",
      "[CV 4/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=nan total time=  34.6s\n",
      "[CV 5/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 5/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=nan total time=  34.5s\n",
      "[CV 1/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=nan total time=  34.2s\n",
      "[CV 2/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=nan total time=  33.4s\n",
      "[CV 3/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=nan total time=  34.6s\n",
      "[CV 4/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=nan total time=  35.3s\n",
      "[CV 5/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=nan total time=  33.4s\n",
      "[CV 1/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=nan total time=  59.3s\n",
      "[CV 2/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=nan total time=  33.1s\n",
      "[CV 3/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=nan total time=  32.9s\n",
      "[CV 4/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=nan total time=  32.7s\n",
      "[CV 5/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=nan total time=  32.9s\n",
      "[CV 1/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=nan total time=  32.6s\n",
      "[CV 2/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 2/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=nan total time=  32.5s\n",
      "[CV 3/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=nan total time=  33.3s\n",
      "[CV 4/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=nan total time=  43.4s\n",
      "[CV 5/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=nan total time=  35.5s\n",
      "[CV 1/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=nan total time=  34.5s\n",
      "[CV 2/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=nan total time=  34.4s\n",
      "[CV 3/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=nan total time=  35.1s\n",
      "[CV 4/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=nan total time=  37.1s\n",
      "[CV 5/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=nan total time=  33.3s\n",
      "[CV 1/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=nan total time=  35.3s\n",
      "[CV 2/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=nan total time=  35.5s\n",
      "[CV 3/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=nan total time=  35.8s\n",
      "[CV 4/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=nan total time=  34.8s\n",
      "[CV 5/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=nan total time=  33.0s\n",
      "[CV 1/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 1/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=nan total time=  34.4s\n",
      "[CV 2/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=nan total time=  37.2s\n",
      "[CV 3/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=nan total time=  35.1s\n",
      "[CV 4/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=nan total time=  35.9s\n",
      "[CV 5/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=nan total time=  32.6s\n",
      "[CV 1/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 1/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=nan total time=  34.0s\n",
      "[CV 2/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=nan total time=  32.5s\n",
      "[CV 3/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 3/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=nan total time=  32.8s\n",
      "[CV 4/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 4/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=nan total time=  32.8s\n",
      "[CV 5/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 5/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=nan total time=  33.3s\n",
      "[CV 1/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 1/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=nan total time=  34.2s\n",
      "[CV 2/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 2/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=nan total time=  34.3s\n",
      "[CV 3/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 3/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=nan total time=  34.7s\n",
      "[CV 4/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 4/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=nan total time=  32.8s\n",
      "[CV 5/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 5/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=nan total time=  33.0s\n",
      "[CV 1/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 1/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=nan total time=  34.2s\n",
      "[CV 2/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 2/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=nan total time=  34.6s\n",
      "[CV 3/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 3/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=nan total time=  34.6s\n",
      "[CV 4/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 4/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=nan total time=  39.6s\n",
      "[CV 5/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 5/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=nan total time=  35.8s\n",
      "[CV 1/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=nan total time=  35.1s\n",
      "[CV 2/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=nan total time=  35.2s\n",
      "[CV 3/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=nan total time=  29.5s\n",
      "[CV 4/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=nan total time=  24.1s\n",
      "[CV 5/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=nan total time=  44.5s\n",
      "[CV 1/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=nan total time=  22.7s\n",
      "[CV 2/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=nan total time=  22.3s\n",
      "[CV 3/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=nan total time=  21.5s\n",
      "[CV 4/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=nan total time=  23.0s\n",
      "[CV 5/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=nan total time=  21.8s\n",
      "[CV 1/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 1/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=nan total time=  27.1s\n",
      "[CV 2/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 2/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=nan total time=  22.5s\n",
      "[CV 3/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 3/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=nan total time=  26.6s\n",
      "[CV 4/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 4/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=nan total time=  21.7s\n",
      "[CV 5/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 5/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=nan total time=  24.0s\n",
      "[CV 1/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=nan total time=  25.3s\n",
      "[CV 2/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=nan total time=  24.1s\n",
      "[CV 3/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=nan total time=  23.9s\n",
      "[CV 4/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=nan total time=  23.2s\n",
      "[CV 5/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=nan total time=  24.0s\n",
      "[CV 1/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=nan total time=  23.4s\n",
      "[CV 2/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=nan total time=  23.6s\n",
      "[CV 3/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=nan total time=  23.5s\n",
      "[CV 4/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=nan total time=  27.8s\n",
      "[CV 5/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=nan total time=  23.2s\n",
      "[CV 1/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 1/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=nan total time=  24.2s\n",
      "[CV 2/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 2/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=nan total time=  24.9s\n",
      "[CV 3/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 3/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=nan total time=  22.8s\n",
      "[CV 4/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 4/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=nan total time=  21.9s\n",
      "[CV 5/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 5/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=nan total time=  21.6s\n",
      "[CV 1/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 1/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=nan total time=  22.1s\n",
      "[CV 2/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 2/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=nan total time=  21.8s\n",
      "[CV 3/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 3/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=nan total time=  21.6s\n",
      "[CV 4/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 4/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=nan total time=  21.9s\n",
      "[CV 5/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=nan total time=  21.7s\n",
      "[CV 1/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 1/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=nan total time=  24.4s\n",
      "[CV 2/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 2/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=nan total time=  23.7s\n",
      "[CV 3/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 3/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=nan total time=  23.5s\n",
      "[CV 4/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 4/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=nan total time=  24.1s\n",
      "[CV 5/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 5/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=nan total time=  24.7s\n",
      "[CV 1/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 1/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=nan total time=  22.9s\n",
      "[CV 2/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 2/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=nan total time=  22.2s\n",
      "[CV 3/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 3/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=nan total time=  21.9s\n",
      "[CV 4/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 4/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=nan total time=  22.2s\n",
      "[CV 5/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 5/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=nan total time=  21.8s\n",
      "[CV 1/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  32.8s\n",
      "[CV 2/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  32.9s\n",
      "[CV 3/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  34.0s\n",
      "[CV 4/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  39.3s\n",
      "[CV 5/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=nan total time=  33.6s\n",
      "[CV 1/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  34.2s\n",
      "[CV 2/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  34.6s\n",
      "[CV 3/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  35.1s\n",
      "[CV 4/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  34.8s\n",
      "[CV 5/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=nan total time=  36.3s\n",
      "[CV 1/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  36.5s\n",
      "[CV 2/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  36.7s\n",
      "[CV 3/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  33.8s\n",
      "[CV 4/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  36.5s\n",
      "[CV 5/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=nan total time=  37.1s\n",
      "[CV 1/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  46.2s\n",
      "[CV 2/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  32.5s\n",
      "[CV 3/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=nan total time=  33.7s\n",
      "[CV 4/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n",
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eba767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
